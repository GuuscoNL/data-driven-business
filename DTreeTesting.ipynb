{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeren gebruikte libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import plot_tree\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_pickle(\"data/ole_model_df.pkl\")\n",
    "train_df = pd.read_pickle(\"data/ole_train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"data/ole_test_df.pkl\")\n",
    "\n",
    "X_train = train_df.drop('progfh_inv_tot_fh', axis=1)\n",
    "y_train = train_df['progfh_inv_tot_fh']\n",
    "\n",
    "X_test = test_df.drop('progfh_inv_tot_fh', axis=1)\n",
    "y_test = test_df['progfh_inv_tot_fh']\n",
    "\n",
    "X = pd.concat([X_train, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_baseline(df):\n",
    "    baseline = df['progfh_inv_tot_fh'].mean()\n",
    "\n",
    "    y_pred = [baseline] * len(df)\n",
    "    y_true = df['progfh_inv_tot_fh']\n",
    "\n",
    "    baseline_rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    baseline_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return baseline_rmse, baseline_r2\n",
    "\n",
    "baseline_rmse, baseline_r2 = calculate_baseline(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_per_leaf_node(tree, X, y):\n",
    "    leaf_indices = tree.apply(X)\n",
    "\n",
    "    rmse_per_leaf = {}\n",
    "\n",
    "    for i in range(tree.tree_.node_count):\n",
    "        # Check if the node is a leaf node\n",
    "        if tree.tree_.children_left[i] == tree.tree_.children_right[i] == -1:\n",
    "            # Get the indices of samples in the current leaf node\n",
    "            indices_in_leaf = np.where(leaf_indices == i)[0]\n",
    "            # If the leaf node is not empty\n",
    "            if len(indices_in_leaf) > 0:\n",
    "                # Calculate RMSE for the leaf node\n",
    "                predictions = tree.tree_.value[i][0][0]\n",
    "                rmse_per_leaf[i] = np.sqrt(mean_squared_error(y[indices_in_leaf], [predictions] * len(indices_in_leaf)))\n",
    "\n",
    "    return rmse_per_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_leaf_rmse(clf: DecisionTreeRegressor) -> dict:\n",
    "    \"\"\"\n",
    "    Deze functie geeft de beste leaf node en de bijbehorende RMSE terug.\n",
    "    \"\"\"\n",
    "    leaf_nodes = [i for i in range(clf.tree_.node_count) if clf.tree_.children_left[i] == clf.tree_.children_right[i]]\n",
    "    rmse_scores = []\n",
    "\n",
    "    for leaf in leaf_nodes:\n",
    "        n_samples_in_node = clf.tree_.n_node_samples[leaf]\n",
    "        if n_samples_in_node > 0:\n",
    "            node_rmse = sqrt(clf.tree_.impurity[leaf])\n",
    "            rmse_scores.append(node_rmse)\n",
    "\n",
    "    return {\n",
    "        'best_leaf': leaf_nodes[np.argmin(rmse_scores)],\n",
    "        'rmse': np.min(rmse_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(\n",
    "        max_depths: list, \n",
    "        min_samples_leafs: list, \n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        X_test: pd.DataFrame\n",
    "        ) -> dict:\n",
    "    \"\"\"\n",
    "    Deze functie vindt de beste hyperparameters voor de DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "    best_rmse = 100000\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    for max_depth in tqdm(max_depths):\n",
    "            for min_samples_leaf in min_samples_leafs:\n",
    "                clf = DecisionTreeRegressor(\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42,\n",
    "                    criterion='squared_error',\n",
    "                )\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "                rmse = get_best_leaf_rmse(clf)['rmse']\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_hyperparameters = {\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_leaf': min_samples_leaf\n",
    "                    }\n",
    "    \n",
    "    return best_hyperparameters\n",
    "\n",
    "find_best_hyperparameters(\n",
    "    max_depths=[i for i in range(1, 21)],\n",
    "    min_samples_leafs=[500, 750, 1000, 1250, 2000],\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 16) \n",
    "\n",
    "train_rmse, test_rmse = [], []\n",
    "train_r2, test_r2 = [], []\n",
    "\n",
    "# Train DTR model met verschillende max_depths\n",
    "for depth in tqdm(depths):\n",
    "    regressor = DecisionTreeRegressor(\n",
    "        max_depth=depth, \n",
    "        min_samples_leaf=500, \n",
    "        criterion='squared_error', \n",
    "        random_state=42\n",
    "        )\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Voorspellingen op de train set\n",
    "    train_predictions = regressor.predict(X_train)\n",
    "    train_rmse.append(sqrt(mean_squared_error(y_train, train_predictions)))\n",
    "    train_r2.append(r2_score(y_train, train_predictions))\n",
    "\n",
    "    # Voorspellingen op de test set\n",
    "    test_predictions = regressor.predict(X_test)\n",
    "    test_rmse.append(sqrt(mean_squared_error(y_test, test_predictions)))\n",
    "    test_r2.append(r2_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot RMSE\n",
    "ax1.plot(depths, train_rmse, marker='o', linestyle='-', color='b', label='Train RMSE')\n",
    "ax1.plot(depths, test_rmse, marker='o', linestyle='-', color='r', label='Test RMSE')\n",
    "ax1.set_title('Depth vs. RMSE voor Decision Tree Regressor')\n",
    "ax1.set_xlabel('Max Depth')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_xticks(depths)\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot R2 score\n",
    "ax2.plot(depths, train_r2, marker='o', linestyle='-', color='b', label='Train R2')\n",
    "ax2.plot(depths, test_r2, marker='o', linestyle='-', color='r', label='Test R2')\n",
    "ax2.set_title('Depth vs. R2 voor Decision Tree Regressor')\n",
    "ax2.set_xlabel('Max Depth')\n",
    "ax2.set_ylabel('R2')\n",
    "ax2.set_xticks(depths)\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "regressor = DecisionTreeRegressor(\n",
    "    max_depth=max_depth, \n",
    "    min_samples_leaf=500, \n",
    "    criterion='squared_error', \n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "print(\"R-squared (R2) Score: \", r2)\n",
    "\n",
    "print('Baseline RMSE: ', baseline_rmse)\n",
    "print('Baseline R2: ', baseline_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"test RMSE's for leaves:\\n {rmse_per_leaf_node(regressor, X_test, y_test)}\")\n",
    "print(f\"train RMSE's of leaves:\\n {rmse_per_leaf_node(regressor, X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "scores = cross_val_score(regressor, X, y, cv=NUM_FOLDS, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(\"Cross-validation RMSE scores: \", -scores)\n",
    "print(\"Mean RMSE: \", -scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12)) \n",
    "plot_tree(regressor, filled=True, proportion=True, impurity=True, precision=2, feature_names=list(X.columns), node_ids=True)\n",
    "plt.title(\"Decision Tree Regressor\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
