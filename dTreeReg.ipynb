{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "In dit notebook zal een Decision Tree Regressor model gebruikt worden om de duur van een storing te voorspellen. Dit wordt gedaan met feature variabelen die gevonden en geprepareerd zijn in \"DataPrep.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importeren gebruikte libraries\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from baseline import calculate_baseline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import plot_tree\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inladen data (al geprepareerd in ander bestand)\n",
    "model_df = pd.read_pickle('data/model_df.pkl')\n",
    "model_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er wordt een Decision Tree Regressor getraint met de features eerder geprepareerd. Eerst wordt er een test-train split gemaakt om het model mee te trainen en mee te testen. Daarna worden er modellen getraint met verschillende max_depths. Dit wordt geplot en in deze plots is te zien wat een goede depth is voor het model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop('anm_tot_fh', axis=1)\n",
    "y = model_df['anm_tot_fh']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 16) \n",
    "\n",
    "rmse = []\n",
    "r2 = []\n",
    "\n",
    "# Train DTR model met verschillende max_depths\n",
    "for depth in tqdm(depths):\n",
    "    regressor = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=0.05, criterion='squared_error', random_state=42)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    predictions = regressor.predict(X_test)\n",
    "    rmse.append(sqrt(mean_squared_error(y_test, predictions)))\n",
    "    r2.append(r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder wordt gekeken wat voor soort diepte goed is voor dit model. We pakken hier een max_depth van 10, omdat de grafiek hier afvlakt. Met een hogere max_depth zal het model snel overfit raken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two plots side by side, first one showing RMSE and second one showing R2 score\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(depths, rmse, marker='o', linestyle='-', color='b')\n",
    "ax1.set_title('Depth vs. RMSE for Decision Tree Regressor')\n",
    "ax1.set_xlabel('Max Depth')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_xticks(depths)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(depths, r2, marker='o', linestyle='-', color='b')\n",
    "ax2.set_title('Depth vs. R2 for Decision Tree Regressor')\n",
    "ax2.set_xlabel('Max Depth')\n",
    "ax2.set_ylabel('R2')\n",
    "ax2.set_xticks(depths)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train het model met de gevonden max_depth. Het model wordt gepickled en opgeslagen zodat deze gerbuikt kan worden in de GUI. Daarna wordt het model vergeleken met de baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 4\n",
    "regressor = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=0.05, criterion='squared_error', random_state=42)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# pickle a regressor model and create new file if it doesn't exist\n",
    "with open('models/DecisionTreeRegressor.pkl', 'wb') as file:\n",
    "    pickle.dump(regressor, file)\n",
    "\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "print(\"R-squared (R2) Score: \", r2)\n",
    "\n",
    "baseline_rmse, baseline_r2 = calculate_baseline(model_df)\n",
    "print('Baseline RMSE: ', baseline_rmse)\n",
    "print('Baseline R2: ', baseline_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie\n",
    "\n",
    "Het model is met een minimaal verschil beter dan de baseline (de RMSE is een heel klein beetje lager en de R2 score is een heel klein beetje hoger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor Probability Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bereken probability van een DTR met mean squared error (als standaard deviatie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rmse for every leaf in the tree\n",
    "leaf_nodes = [i for i in range(regressor.tree_.node_count) if regressor.tree_.children_left[i] == regressor.tree_.children_right[i]]\n",
    "\n",
    "rmse_per_leaf = {}\n",
    "\n",
    "for idx in leaf_nodes:\n",
    "    samples_in_node = regressor.tree_.n_node_samples[idx]\n",
    "    if samples_in_node > 0:\n",
    "        node_rmse = sqrt(regressor.tree_.impurity[idx] * samples_in_node / (samples_in_node + 1)) \n",
    "        rmse_per_leaf[idx] = node_rmse\n",
    "        print(\"Leaf Node {} has RMSE {}\".format(idx, node_rmse))\n",
    "        \n",
    "# Get the models prediction per leaf\n",
    "pred_per_leaf = {idx: regressor.tree_.value[idx][0][0] for idx, _ in rmse_per_leaf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 12)) \n",
    "# plot_tree(regressor, filled=True, proportion=True, impurity=False, precision=2, feature_names=X.columns, node_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_indices = regressor.apply(X)\n",
    "\n",
    "samples_in_leaves = {}\n",
    "\n",
    "# Iterate over unique leaf indices\n",
    "unique_leaf_indices = np.unique(leaf_indices)\n",
    "for leaf_index in unique_leaf_indices:\n",
    "    # Select the target values (y) that belong to the current leaf\n",
    "    samples_in_leaves[leaf_index] = y[leaf_indices == leaf_index].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf = 4\n",
    "durations = np.array(samples_in_leaves[leaf])\n",
    "mean_prediction = pred_per_leaf[leaf]\n",
    "\n",
    "percentile_95 = np.percentile(durations, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "_, bins, _ = ax.hist(durations, bins=30, density=False, alpha=0.6, color='b', label='Historische storingsduur Data')\n",
    "ax.axvline(mean_prediction, color='r', linestyle='--', label=f'Mean Prediction = {mean_prediction:.0f}')\n",
    "ax.axvline(percentile_95, color='g', linestyle='--', label='95% Mark', linewidth=2)\n",
    "ax.set_xlabel('Duur storing (minuten)')\n",
    "ax.set_ylabel('Frequentie')\n",
    "ax.set_title('Storingsduur histogram')\n",
    "\n",
    "values_below_mean_prediction = durations[durations < mean_prediction]\n",
    "values_above_mean_prediction = durations[durations >= mean_prediction]\n",
    "percentage_below_mean = len(values_below_mean_prediction) / len(durations) * 100\n",
    "percentage_above_mean = len(values_above_mean_prediction) / len(durations) * 100\n",
    "\n",
    "right_side_color = 'orange'\n",
    "n, bins, patches = ax.hist(values_above_mean_prediction, bins=bins, alpha=0.6, color=right_side_color, label=f'Values Above Mean Prediction: {percentage_above_mean:.2f}%')\n",
    "\n",
    "labels = [\n",
    "    f'Waardes onder voorspelling: {percentage_below_mean:.2f}%', \n",
    "    f'Voorspelling: {mean_prediction:.2f} min',\n",
    "    f'95% van de data: {percentile_95:.2f} min', \n",
    "    f'Waardes boven voorspelling: {percentage_above_mean:.2f}%'\n",
    "    ]\n",
    "ax.legend(labels=labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.PlotPrediction import plot_prediction\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "plot_prediction(regressor, X_test.iloc[0:0+1], ax, X, y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
